dataloader_args:
  batch_size: 128
  drop_last: true
  num_workers: 8
  pin_memory: false
  prefetch_factor: 6
dataset_args:
  aug_prob: 0.6
  cmvn: true
  cmvn_args:
    norm_mean: true
    norm_var: false
  filter: true
  filter_args:
    max_num_frames: 400
    min_num_frames: 50
  frontend: huggingface
  huggingface_args:
    frame_length: 20
    frame_shift: 20
    frozen: false
    upstream_args:
      name: wavlm_base_plus
      path_or_url: /scratch/project_465002053/junyi/sv/wespeaker_dev/wespeaker_hubert/examples/voxceleb/v4_pruning/convert/wavlm_base_plus.hf.pth
      pruning_units: ''
  num_frms: 150
  resample_rate: 16000
  sample_num_per_epoch: 0
  shuffle: true
  shuffle_args:
    shuffle_size: 2500
  spec_aug: false
  spec_aug_args:
    max_f: 8
    max_t: 10
    num_f_mask: 1
    num_t_mask: 1
    prob: 0.6
  speed_perturb: false
do_lm: false
enable_amp: false
exp_dir: exp/ECAPA_TDNN_GLOB_c512-ASTP-emb192-WavLM_Large_joint_ft-num_frms150-aug0.6-spTrue-saFalse-ArcMargin_intertopk_subcenter-SGD-epoch20
freeze_lsq_steps: 5000
gpus: '[0,1,2,3,4,5,6,7]'
grad_clip_norm: 0.1
initial_lr: 5.0e-05
initial_reg_lr: 0.005
log_batch_interval: 100
loss: CrossEntropyLoss
loss_args: {}
lsq_step_lr: 1.0e-07
margin_scheduler: MarginScheduler
margin_update:
  final_margin: 0.2
  fix_start_epoch: 1
  increase_start_epoch: 1
  increase_type: exp
  initial_margin: 0.2
  update_margin: true
min_sparsity: 0.0
model: SSL_BACKEND_MHFA
model_args:
  compression_dim: 128
  embed_dim: 256
  feat_dim: -1
  feature_grad_mult: 0.1
  head_nb: 32
  nb_layer: 13
model_init: null
num_avg: 2
num_epochs: 12
optimizer: AdamW
optimizer_args:
  weight_decay: 1.0e-07
per_channel_activations: false
per_channel_weights: true
preserve_hp_gating: true
projection_args:
  easy_margin: false
  project_type: arc_margin_intertopk_subcenter
  scale: 32.0
quantization_config: 1bit_symmetric
quantize_activations: false
quantize_bias: false
quantize_weights: true
save_epoch_interval: 1
scheduler: ExponentialDecrease
scheduler_args:
  final_lr: 1.0e-06
  initial_lr: 0.0001
  warm_from_zero: true
  warm_up_epoch: 5
seed: 42
sparsity_schedule: cosine
sparsity_warmup_epochs: 8
target_sparsity: 0.5
use_pruning_loss: true
use_quantization: true
